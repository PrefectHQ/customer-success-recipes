{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cabc458",
   "metadata": {},
   "source": [
    "<img src=\"img/prefect-logo.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f59afe9",
   "metadata": {},
   "source": [
    "# Getting Started with Prefect\n",
    "Data workflow orchestrators are used to coordinate, monitor and observe data movement. Features that make data-pipelines fault-tolerant include:\n",
    "- scheduling and triggering jobs\n",
    "- adding retries\n",
    "- dependency and state depedencies ('if the previous job failed')\n",
    "- caching expensive tasks\n",
    "- deploying flows to different environment \n",
    "- visibility into execution state of all jobs in your workflows\n",
    "\n",
    "Stop safeguarding against failure! Instead, enjoy a single pane of glass for monitoring your code. \n",
    "\n",
    "Let's gain visibility into our Python scripts with Prefect. All we have to do for observability, scheduling and more is to add a single `@flow` decorator (and optionally `@task` decorators). After that, we'll also create a deployment, view it in the UI, add a schedule, and watch our code run in the Prefect UI. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a140db3",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "[Use the README to clone the repo and install Jupyter notebook](./README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbf8920",
   "metadata": {},
   "source": [
    "## Demo agenda\n",
    "1. We will install dependencies.\n",
    "2. We will take a look at our first flow (Python code).\n",
    "3. We will run that flow locally.\n",
    "4. We will then view our flow run in the UI. \n",
    "5. We will deploy our code so that we don't need to run it locally anymore.\n",
    "6. We will add a schedule to our code. \n",
    "7. We will start a local agent that can execute our code on our schedule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200e50dd",
   "metadata": {},
   "source": [
    "## 1. First, we'll install Prefect and some other dependencies our script has to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df94a551",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install prefect==2.3.1 prefect-dask sklearn pandas\n",
    "# The magic capture cmd simply suppresses the install output, which is lengthy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41892af",
   "metadata": {},
   "source": [
    "## 2. Our first flow\n",
    "Let's write a flow that takes data from the Titanic dataset and sees which model most accurately predicts the ship survivors. Read through the code in the cell below, noting that almost the only difference is the addition of `@task` and `@flow` decorators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6472524b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import Str\n",
    "from datetime import timedelta\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "from prefect import flow, tags, task\n",
    "from prefect.tasks import task_input_hash\n",
    "from prefect_dask.task_runners import DaskTaskRunner\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "@task(name=\"create-data\", description=\"This task reads in and wrangles Titanic data\")\n",
    "def create_data():\n",
    "    \"\"\"\n",
    "    Task that reads in data from the CSV, then cleans it. Easily extensible to other\n",
    "    methods of data extraction, from an S3 bucket, API, etc.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(\"titanic.csv\")\n",
    "    df = df.drop([\"Name\"], axis=1)\n",
    "    df[\"Sex\"] = pd.factorize(df[\"Sex\"])[0]\n",
    "    y = df[\"Survived\"]\n",
    "    X = df.drop(\"Survived\", axis=1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    fill_age = X_train[\"Age\"].mean()\n",
    "    X_train[\"Age\"] = X_train[\"Age\"].fillna(fill_age)\n",
    "    X_test[\"Age\"] = X_test[\"Age\"].fillna(fill_age)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "@task(name=\"get-models\", description=\"Retrieve models with hyperparams\", retries=2)\n",
    "def get_models(n_estimators=200) -> List:\n",
    "    \"\"\"\n",
    "    A task that retrieves the models to be used. We have hard-coded them,\n",
    "    but you can specify other models!\n",
    "    \"\"\"\n",
    "    return [\n",
    "        LogisticRegression(random_state=42),\n",
    "        KNeighborsClassifier(),\n",
    "        DecisionTreeClassifier(),\n",
    "        SVC(),\n",
    "        RandomForestClassifier(n_estimators=n_estimators, max_depth=4, random_state=42),\n",
    "        RandomForestClassifier(n_estimators=100, max_depth=3, random_state=42),\n",
    "    ]\n",
    "\n",
    "\n",
    "@task(name=\"train-models\", description=\"Use models to train and predict with\")\n",
    "def train_model(\n",
    "    model: Any, X_train: DataFrame, X_test: DataFrame, y_train: Series, y_test: Series\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    This task allows us to use some sklearn to easily train a variety of models\n",
    "    and output an accuracy score and the params used by that model.\n",
    "    \"\"\"\n",
    "    clf = model.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    return {\n",
    "        \"model\": model.__class__.__name__,\n",
    "        \"params\": model.get_params(),\n",
    "        \"accuracy\": acc,\n",
    "    }\n",
    "\n",
    "\n",
    "@task(cache_key_fn=task_input_hash, cache_expiration=timedelta(days=1))\n",
    "def get_results(results: Dict) -> Str:\n",
    "    \"\"\"\n",
    "    In this task we have added specification to cache the results\n",
    "    if they are unchanged. The cache expires in 1 day, as we specified.\n",
    "    \"\"\"\n",
    "    res = pd.DataFrame(results)\n",
    "    return res\n",
    "\n",
    "\n",
    "@flow(name=\"my-first-ml-flow\", task_runner=DaskTaskRunner())\n",
    "def my_first_ml_flow(n_estimators=200):\n",
    "    \"\"\"\n",
    "    This flow will allow us to run all of the tasks that we defined above\n",
    "    as we call them within this function.\n",
    "    \"\"\"\n",
    "    with tags(\"dev\"):  # optional tag specification\n",
    "        # call each task\n",
    "        X_train, X_test, y_train, y_test = create_data()\n",
    "        models = get_models(n_estimators)\n",
    "        training_runs = [\n",
    "            train_model(model, X_train, X_test, y_train, y_test) for model in models\n",
    "        ]\n",
    "        model_results = get_results(training_runs)\n",
    "\n",
    "        return model_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25044d50",
   "metadata": {},
   "source": [
    "## 3. Let's run the flow to see the logs that are generated from this run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f571e734",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:05:53.607 | INFO    | prefect.engine - Created flow run 'whispering-oriole' for flow 'my-first-ml-flow'\n",
      "14:05:53.608 | INFO    | prefect.task_runner.dask - Creating a new Dask cluster with `distributed.deploy.local.LocalCluster`\n",
      "14:05:54.796 | INFO    | prefect.task_runner.dask - The Dask dashboard is available at http://127.0.0.1:8787/status\n",
      "14:05:55.012 | INFO    | Flow run 'whispering-oriole' - Created task run 'create-data-aa7d62a6-0' for task 'create-data'\n",
      "14:05:55.013 | INFO    | Flow run 'whispering-oriole' - Executing 'create-data-aa7d62a6-0' immediately...\n",
      "14:05:55.061 | INFO    | Task run 'create-data-aa7d62a6-0' - Finished in state Completed()\n",
      "14:05:55.075 | INFO    | Flow run 'whispering-oriole' - Created task run 'get-models-e133abf5-0' for task 'get-models'\n",
      "14:05:55.076 | INFO    | Flow run 'whispering-oriole' - Executing 'get-models-e133abf5-0' immediately...\n",
      "14:05:55.130 | INFO    | Task run 'get-models-e133abf5-0' - Finished in state Completed()\n",
      "14:05:55.152 | INFO    | Flow run 'whispering-oriole' - Created task run 'train-models-d57f6317-0' for task 'train-models'\n",
      "14:05:55.152 | INFO    | Flow run 'whispering-oriole' - Executing 'train-models-d57f6317-0' immediately...\n",
      "14:05:55.203 | INFO    | Task run 'train-models-d57f6317-0' - Finished in state Completed()\n",
      "14:05:55.223 | INFO    | Flow run 'whispering-oriole' - Created task run 'train-models-d57f6317-1' for task 'train-models'\n",
      "14:05:55.224 | INFO    | Flow run 'whispering-oriole' - Executing 'train-models-d57f6317-1' immediately...\n",
      "14:05:55.264 | INFO    | Task run 'train-models-d57f6317-1' - Finished in state Completed()\n",
      "14:05:55.289 | INFO    | Flow run 'whispering-oriole' - Created task run 'train-models-d57f6317-2' for task 'train-models'\n",
      "14:05:55.289 | INFO    | Flow run 'whispering-oriole' - Executing 'train-models-d57f6317-2' immediately...\n",
      "14:05:55.341 | INFO    | Task run 'train-models-d57f6317-2' - Finished in state Completed()\n",
      "14:05:55.360 | INFO    | Flow run 'whispering-oriole' - Created task run 'train-models-d57f6317-3' for task 'train-models'\n",
      "14:05:55.361 | INFO    | Flow run 'whispering-oriole' - Executing 'train-models-d57f6317-3' immediately...\n",
      "14:05:55.414 | INFO    | Task run 'train-models-d57f6317-3' - Finished in state Completed()\n",
      "14:05:55.430 | INFO    | Flow run 'whispering-oriole' - Created task run 'train-models-d57f6317-4' for task 'train-models'\n",
      "14:05:55.430 | INFO    | Flow run 'whispering-oriole' - Executing 'train-models-d57f6317-4' immediately...\n",
      "14:05:55.621 | INFO    | Task run 'train-models-d57f6317-4' - Finished in state Completed()\n",
      "14:05:55.641 | INFO    | Flow run 'whispering-oriole' - Created task run 'train-models-d57f6317-5' for task 'train-models'\n",
      "14:05:55.642 | INFO    | Flow run 'whispering-oriole' - Executing 'train-models-d57f6317-5' immediately...\n",
      "14:05:55.754 | INFO    | Task run 'train-models-d57f6317-5' - Finished in state Completed()\n",
      "14:05:55.770 | INFO    | Flow run 'whispering-oriole' - Created task run 'get_results-3af99611-0' for task 'get_results'\n",
      "14:05:55.771 | INFO    | Flow run 'whispering-oriole' - Executing 'get_results-3af99611-0' immediately...\n",
      "14:05:55.791 | INFO    | Task run 'get_results-3af99611-0' - Finished in state Cached(, type=COMPLETED)\n",
      "14:05:57.006 | INFO    | Flow run 'whispering-oriole' - Finished in state Completed()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>params</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>{'C': 1.0, 'class_weight': None, 'dual': False...</td>\n",
       "      <td>0.752809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>{'algorithm': 'auto', 'leaf_size': 30, 'metric...</td>\n",
       "      <td>0.691011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>{'ccp_alpha': 0.0, 'class_weight': None, 'crit...</td>\n",
       "      <td>0.735955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVC</td>\n",
       "      <td>{'C': 1.0, 'break_ties': False, 'cache_size': ...</td>\n",
       "      <td>0.651685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>{'bootstrap': True, 'ccp_alpha': 0.0, 'class_w...</td>\n",
       "      <td>0.792135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    model                                             params  \\\n",
       "0      LogisticRegression  {'C': 1.0, 'class_weight': None, 'dual': False...   \n",
       "1    KNeighborsClassifier  {'algorithm': 'auto', 'leaf_size': 30, 'metric...   \n",
       "2  DecisionTreeClassifier  {'ccp_alpha': 0.0, 'class_weight': None, 'crit...   \n",
       "3                     SVC  {'C': 1.0, 'break_ties': False, 'cache_size': ...   \n",
       "4  RandomForestClassifier  {'bootstrap': True, 'ccp_alpha': 0.0, 'class_w...   \n",
       "\n",
       "   accuracy  \n",
       "0  0.752809  \n",
       "1  0.691011  \n",
       "2  0.735955  \n",
       "3  0.651685  \n",
       "4  0.792135  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_first_ml_flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86e39f9",
   "metadata": {},
   "source": [
    "As we can see, info logs were generated about this flow run. We also have a flow run name! For example, your flow run name might be 'practical peacock`. Each flow run name is unique. But now let's kick thing up a notch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e428a5e",
   "metadata": {},
   "source": [
    "## 4. View your flow run in the UI\n",
    "1. Copy this command: `prefect orion start`, which will allow you to start an Orion server, enabling you to view your flow!\n",
    "2. Visit the `Jupyter Home Page` tab in your browser. Click \"NEW\" in the right-hand corner and choose \"TERMINAL\". \n",
    "3. This will open a new terminal window. Paste the command.\n",
    "4. Visit http://127.0.0.1:4200. In the `Flow Runs` tab, you should see your flow run name (e.g. 'practical peacock'), which you can click on view its logs, task runs, parameters, execution state, and more.\n",
    "\n",
    "<img src=\"img/viewing-flow-run.png\" style=\"width: 900px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3770e7",
   "metadata": {},
   "source": [
    "## 5. Create a deployment\n",
    "Now that you have run a flow and viewed it in the UI, let's create a deployment. Deploying your flow means you no longer need to call the function, flow or .py file locally to run your code. With a single command, any script you write in Python can be executed regularly while enabling you to be notified of failures and observe the state of your jobs at all times.\n",
    "\n",
    "Because deployments are created from .py files (and we're in a .ipynb), we'll need to write our flow out to a file first by executing the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2bb60fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefect flow written out to prefect_ml.py.\n"
     ]
    }
   ],
   "source": [
    "def write_flow_to_file(filename: str=\"prefect_ml.py\"):\n",
    "    \"\"\"\n",
    "    This step is only necessary when creating a Prefect deployment in a Jupyter notebook.\n",
    "    Prefect deployments are built from .py files. So we'll write our code out to filename.py\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write('''\\\n",
    "    from ast import Str\n",
    "    from datetime import timedelta\n",
    "    from typing import Any, Dict, List\n",
    "\n",
    "    import pandas as pd\n",
    "    from pandas import DataFrame, Series\n",
    "    from prefect import flow, tags, task\n",
    "    from prefect.tasks import task_input_hash\n",
    "    from prefect_dask.task_runners import DaskTaskRunner\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "    @task(name=\"create-data\", description=\"This task reads in and wrangles Titanic data\")\n",
    "    def create_data():\n",
    "        \"\"\"\n",
    "        Task that reads in data from the CSV, then cleans it. Easily extensible to other\n",
    "        methods of data extraction, from an S3 bucket, API, etc.\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(\"titanic.csv\")\n",
    "        df = df.drop([\"Name\"], axis=1)\n",
    "        df[\"Sex\"] = pd.factorize(df[\"Sex\"])[0]\n",
    "        y = df[\"Survived\"]\n",
    "        X = df.drop(\"Survived\", axis=1)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "        fill_age = X_train[\"Age\"].mean()\n",
    "        X_train[\"Age\"] = X_train[\"Age\"].fillna(fill_age)\n",
    "        X_test[\"Age\"] = X_test[\"Age\"].fillna(fill_age)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "    @task(name=\"get-models\", description=\"Retrieve models with hyperparams\", retries=2)\n",
    "    def get_models(n_estimators=200) -> List:\n",
    "        \"\"\"\n",
    "        A task that retrieves the models to be used. We have hard-coded them,\n",
    "        but you can specify other models!\n",
    "        \"\"\"\n",
    "        return [\n",
    "            LogisticRegression(random_state=42),\n",
    "            KNeighborsClassifier(),\n",
    "            DecisionTreeClassifier(),\n",
    "            SVC(),\n",
    "            RandomForestClassifier(n_estimators=n_estimators, max_depth=4, random_state=42),\n",
    "            RandomForestClassifier(n_estimators=100, max_depth=3, random_state=42),\n",
    "        ]\n",
    "\n",
    "\n",
    "    @task(name=\"train-models\", description=\"Use models to train and predict with\")\n",
    "    def train_model(\n",
    "        model: Any, X_train: DataFrame, X_test: DataFrame, y_train: Series, y_test: Series\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        This task allows us to use some sklearn to easily train a variety of models\n",
    "        and output an accuracy score and the params used by that model.\n",
    "        \"\"\"\n",
    "        clf = model.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        return {\n",
    "            \"model\": model.__class__.__name__,\n",
    "            \"params\": model.get_params(),\n",
    "            \"accuracy\": acc,\n",
    "        }\n",
    "\n",
    "\n",
    "    @task(cache_key_fn=task_input_hash, cache_expiration=timedelta(days=1))\n",
    "    def get_results(results: Dict) -> Str:\n",
    "        \"\"\"\n",
    "        In this task we have added specification to cache the results\n",
    "        if they are unchanged. The cache expires in 1 day, as we specified.\n",
    "        \"\"\"\n",
    "        res = pd.DataFrame(results)\n",
    "        return res\n",
    "\n",
    "\n",
    "    @flow(name=\"my-first-ml-flow\", task_runner=DaskTaskRunner())\n",
    "    def my_first_ml_flow(n_estimators=200):\n",
    "        \"\"\"\n",
    "        This flow will allow us to run all of the tasks that we defined above\n",
    "        as we call them within this function.\n",
    "        \"\"\"\n",
    "        with tags(\"dev\"):  # optional tag specification\n",
    "            # call each task\n",
    "            X_train, X_test, y_train, y_test = create_data()\n",
    "            models = get_models(n_estimators)\n",
    "            training_runs = [\n",
    "                train_model(model, X_train, X_test, y_train, y_test) for model in models\n",
    "            ]\n",
    "            model_results = get_results(training_runs)\n",
    "\n",
    "            print(model_results.head())\n",
    "\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        my_first_ml_flow()\n",
    "        ''')\n",
    "    print(f\"Prefect flow written out to {filename}.\")\n",
    "        \n",
    "write_flow_to_file(\"prefect_ml.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfd4983",
   "metadata": {},
   "source": [
    "## Now we're ready to build and deploy in one command!\n",
    "Run the following cell, which will build and apply a deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe7c942d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found flow 'my-first-ml-flow'\n",
      "Deployment YAML created at \n",
      "'/Users/bean/Documents/prefect-in-jupyter-notebook/my_first_ml_flow-deployment.y\n",
      "aml'.\n",
      "Deployment 'my-first-ml-flow/ML Flow Deployment' successfully created with id \n",
      "'3490b13d-8a13-4d85-b490-b7de3bcdf649'.\n",
      "\n",
      "To execute flow runs from this deployment, start an agent that pulls work from \n",
      "the the 'default' work queue:\n",
      "$ prefect agent start -q 'default'\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "prefect deployment build prefect_ml:my_first_ml_flow --name \"ML Flow Deployment\" --apply --skip-upload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ca00b1",
   "metadata": {},
   "source": [
    "We don't need to worry about starting an agent quite yet. For now, let's head on over to the UI to view our newly created deployment!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d78e884",
   "metadata": {},
   "source": [
    "## 6. Add a schedule to your deployment\n",
    "[In the UI](http://127.0.0.1:4200), take a look at the `Deployments` tab. Click on your new deployment (called `ML Flow Deployment`). Add a schedule by clicking \"Add\" and choosing your desired frequency.\n",
    "\n",
    "<img src=\"img/adding-a-schedule.png\" style=\"width: 900px;\"/>\n",
    "\n",
    "Now when you look at the UI, in the `Flow Runs` tab you'll see many scheduled runs of your flow!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ebc6ea",
   "metadata": {},
   "source": [
    "## 7. Execute flow runs from this deployment \n",
    "Starting a local agent will enable those scheduled flow runs to kick off. To start an agent, you simply need to:\n",
    "1. Copy this command: `prefect agent start -q default`.\n",
    "2. Visit the `Jupyter Home Page` tab in your browser. Click \"NEW\" in the right-hand corner and choose \"TERMINAL\". \n",
    "3. This will open a new terminal window. Paste the command.\n",
    "4. With the agent running, at the next schedule flow run the flow will move from a `Scheduled` to `Completed` state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca6528a",
   "metadata": {},
   "source": [
    "## Have some fun\n",
    "Experiment on your own in the UI by adding a description to your deployment, parameters, and much more!\n",
    "\n",
    "While in this demo, we use a static dataset (titanic.csv), it's easy to see how the `create_data()` task could be altered to draw from any number of sources, using the `requests` library, for instance, to request data from an API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb473266",
   "metadata": {},
   "source": [
    "## What's next? \n",
    "Not interested in starting up the Orion server everytime you want to view your flow runs? Check out [Prefect Cloud](https://docs.prefect.io/ui/cloud/), where you can create plenty of projects in your first workspace for free!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcd7cc7",
   "metadata": {},
   "source": [
    "## Viewing or removing metadata\n",
    "You can view metadata about your flows and deployments by cd-ing into your orion.db: `open ~/.prefect/orion.db`. This will open a SQLite browser if one exists.\n",
    "\n",
    "Alternatively, you can `rm -rf ~/.prefect/orion.db` if you would like to remove all of your existing flows, flow runs, and deployments that you created. If you only want to remove one flow, flow run, or deployment, I would recommend doing so through the UI. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a268c5",
   "metadata": {},
   "source": [
    "## Happy engineering!\n",
    "We hope you were able to learn a little more about how Prefect works. We've barely scratched the surface of what's possible with Prefect. For instance, you can also store repeated configuration with blocks and integrate with other tools easily. Please see our docs to learn even more about the possibilities Prefect can give your workflows:\n",
    "https://docs.prefect.io/\n",
    "\n",
    "Some things we didn't touch on in this tutorial:\n",
    "- Other build commands that can be used during deployment: https://docs.prefect.io/concepts/deployments/#deployment-build-options\n",
    "- Flow Storage: https://docs.prefect.io/concepts/storage/\n",
    "- We can set log levels to 'debug', 'error', 'info', and more: https://docs.prefect.io/concepts/logs/\n",
    "- Specifying infrastructure allows you to deploy your agents and control where your flows are run: https://docs.prefect.io/concepts/infrastructure/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e95219b",
   "metadata": {},
   "source": [
    "<img src=\"img/marvin.png\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd591bd",
   "metadata": {},
   "source": [
    "Prefect's mascot, Marvin, is always cooking up new recipes on how to use Prefect. Stop on by [prefect-recipes](https://github.com/PrefectHQ/prefect-recipes) to see more!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "85c41614c0809ea67edb8f40bd66dd48c88720a96ccce3056986d75eb54b153e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
